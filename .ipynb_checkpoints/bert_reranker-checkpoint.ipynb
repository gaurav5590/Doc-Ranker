{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79eebb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification #Can use the texar equivalent here\n",
    "from elasticsearch import Elasticsearch\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e508a775",
   "metadata": {},
   "source": [
    "### Configuration - Goes into config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "534fcbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'data/collectionandqueries/queries.dev.small.tsv'\n",
    "ground_truth_file = 'data/collectionandqueries/qrels.dev.small.tsv'\n",
    "output_file = 'output/results_dev.tsv'\n",
    "\n",
    "host = 'localhost:9200'\n",
    "index_name = 'elastic_index'\n",
    "size = 100 # For testing purposes - Use 1000 for full-ranking\n",
    "\n",
    "model_name = 'amberoad/bert-multilingual-passage-reranking-msmarco'\n",
    "max_seq_length = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450f9015",
   "metadata": {},
   "source": [
    "### Full-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "854fc341",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33b7a8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_search(query_text):\n",
    "    \n",
    "    query_body = {\"query\": {\"match\": {'content': query_text}}, \"size\": size}\n",
    "    results = es.search(index=\"elastic_index\", body=query_body)\n",
    "    hits = results['hits']['hits']\n",
    "\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced694d",
   "metadata": {},
   "source": [
    "### Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b01b9513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "256b05f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query_pack):\n",
    "    \n",
    "    # Get query id and text\n",
    "    query_id = query_pack[0]\n",
    "    query_text = query_pack[1]\n",
    "    \n",
    "    # Get the top-1000 results from es bm25 and keep in doc_packs\n",
    "    hits = es_search(query_text)\n",
    "    \n",
    "    doc_packs = [[hit['_source']['doc_id'], hit['_source']['content']] for hit in hits]\n",
    "    \n",
    "    # ===== Iterate through doc_packs - following the Forte pipeline - Very slow hence vectorized ====\n",
    "#     doc_scores = []\n",
    "#     for doc_pack in doc_packs:\n",
    "        \n",
    "#         doc_id = doc_pack[0]\n",
    "#         doc_text = doc_pack[1]\n",
    "        \n",
    "#         # Bert Inference\n",
    "#         encodings = tokenizer(query_text, doc_text, padding = True, max_length=max_seq_length, return_tensors= 'pt')\n",
    "        \n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             logits = model(**encodings)\n",
    "        \n",
    "#         pt_predictions = F.softmax(logits[0], dim=1)\n",
    "#         score = pt_predictions.tolist()[0][1]\n",
    "        \n",
    "#         doc_scores.append([doc_id, score])\n",
    "    \n",
    "    # Vectorization - Still similarly slow ============================================================\n",
    "    \n",
    "    docs_id = list(list(zip(*doc_packs))[0])\n",
    "    docs_content = list(list(zip(*doc_packs))[1])\n",
    "    \n",
    "    # Bert Inference\n",
    "    encodings = tokenizer([query_text] * len(docs_content), docs_content, \n",
    "                          padding = True, max_length=max_seq_length, return_tensors= 'pt')\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encodings)\n",
    "    \n",
    "    pt_predictions = F.softmax(logits[0], dim=1)\n",
    "    scores = pt_predictions[:,1]\n",
    "    \n",
    "    doc_scores = list(zip(docs_id, scores))\n",
    "    \n",
    "    doc_scores = sorted(doc_scores, key = lambda x: x[1], reverse=True)\n",
    "    doc_ranks = [[query_id, row[0], idx+1] for idx, row in enumerate(doc_scores)]\n",
    "    \n",
    "    return doc_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78199744",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bfc40f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked 1 queries\n",
      "Ranked 2 queries\n",
      "Ranked 3 queries\n",
      "Ranked 4 queries\n",
      "Ranked 5 queries\n",
      "Ranked 6 queries\n",
      "Ranked 7 queries\n",
      "Ranked 8 queries\n",
      "Ranked 9 queries\n",
      "Ranked 10 queries\n",
      "Completed ranking 10 queries\n"
     ]
    }
   ],
   "source": [
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    if not os.path.exists('output'):\n",
    "        os.makedirs('output')\n",
    "    open(output_file, \"w\").close()\n",
    "    \n",
    "    for line in file:\n",
    "        \n",
    "        query_pack = line.split('\\t', 1)\n",
    "        \n",
    "        # Get the ranks after full-ranker and re-ranker\n",
    "        doc_ranks = process_query(query_pack)\n",
    "        \n",
    "        # Append the results to tsv\n",
    "        with open(output_file, 'a', newline='') as f:\n",
    "            tsv_writer = csv.writer(f, delimiter='\\t')\n",
    "            [tsv_writer.writerow(row) for row in doc_ranks]\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        if counter % 1 == 0:\n",
    "            print(f'Ranked {counter} queries')\n",
    "            \n",
    "        # Removing below break will run for all 7k queries\n",
    "        if counter==10:\n",
    "            break\n",
    "        \n",
    "print(f'Completed ranking {counter} queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6902a49a",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "73fe1668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ms_marco_eval import compute_metrics_from_files\n",
    "metrics = compute_metrics_from_files(path_to_reference = ground_truth_file, path_to_candidate = output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "317470c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MRR @10': 0.0005027971073816346, 'QueriesRanked': 10}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66d02ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
