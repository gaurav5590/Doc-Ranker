{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79eebb2c",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification #Can use the texar equivalent here\n",
    "from elasticsearch import Elasticsearch\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e508a775",
   "metadata": {},
   "source": [
    "### Configuration - Goes into config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "534fcbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'data/collectionandqueries/queries.dev.small.tsv'\n",
    "ground_truth_file = 'data/collectionandqueries/qrels.dev.small.tsv'\n",
    "output_file = 'output/results_dev.tsv'\n",
    "\n",
    "host = 'localhost:9200'\n",
    "index_name = 'elastic_index'\n",
    "size = 100 # For testing purposes - Use 1000 for full-ranking\n",
    "\n",
    "model_name = 'amberoad/bert-multilingual-passage-reranking-msmarco'\n",
    "max_seq_length = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450f9015",
   "metadata": {},
   "source": [
    "### Full-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "854fc341",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33b7a8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_search(query_text):\n",
    "    \n",
    "    query_body = {\"query\": {\"match\": {'content': query_text}}, \"size\": size}\n",
    "    results = es.search(index=\"elastic_index\", body=query_body)\n",
    "    hits = results['hits']['hits']\n",
    "\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced694d",
   "metadata": {},
   "source": [
    "### Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b01b9513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "256b05f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query_pack):\n",
    "    \n",
    "    # Get query id and text\n",
    "    query_id = query_pack[0]\n",
    "    query_text = query_pack[1]\n",
    "    \n",
    "    # Get the top-1000 results from es bm25 and keep in doc_packs\n",
    "    hits = es_search(query_text)\n",
    "    \n",
    "    doc_packs = [[hit['_source']['doc_id'], hit['_source']['content']] for hit in hits]\n",
    "    \n",
    "    # ===== Iterate through doc_packs - following the Forte pipeline - Very slow hence vectorized ====\n",
    "#     doc_scores = []\n",
    "#     for doc_pack in doc_packs:\n",
    "        \n",
    "#         doc_id = doc_pack[0]\n",
    "#         doc_text = doc_pack[1]\n",
    "        \n",
    "#         # Bert Inference\n",
    "#         encodings = tokenizer(query_text, doc_text, padding = True, max_length=max_seq_length, return_tensors= 'pt')\n",
    "        \n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             logits = model(**encodings)\n",
    "        \n",
    "#         pt_predictions = F.softmax(logits[0], dim=1)\n",
    "#         score = pt_predictions.tolist()[0][1]\n",
    "        \n",
    "#         doc_scores.append([doc_id, score])\n",
    "    \n",
    "    # Vectorization - Still similarly slow ============================================================\n",
    "    \n",
    "    docs_id = list(list(zip(*doc_packs))[0])\n",
    "    docs_content = list(list(zip(*doc_packs))[1])\n",
    "    \n",
    "    # Bert Inference\n",
    "    encodings = tokenizer([query_text] * len(docs_content), docs_content, \n",
    "                          padding = True, max_length=max_seq_length, return_tensors= 'pt')\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encodings)\n",
    "    \n",
    "    pt_predictions = F.softmax(logits[0], dim=1)\n",
    "    scores = pt_predictions[:,1]\n",
    "    \n",
    "    doc_scores = list(zip(docs_id, scores))\n",
    "    \n",
    "    doc_scores = sorted(doc_scores, key = lambda x: x[1], reverse=True)\n",
    "    doc_ranks = [[query_id, row[0], idx+1] for idx, row in enumerate(doc_scores)]\n",
    "    \n",
    "    return doc_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78199744",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bfc40f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked 1 queries\n",
      "Ranked 2 queries\n",
      "Ranked 3 queries\n",
      "Ranked 4 queries\n",
      "Ranked 5 queries\n",
      "Ranked 6 queries\n",
      "Ranked 7 queries\n",
      "Ranked 8 queries\n",
      "Ranked 9 queries\n",
      "Ranked 10 queries\n",
      "Completed ranking 10 queries\n"
     ]
    }
   ],
   "source": [
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    if not os.path.exists('output'):\n",
    "        os.makedirs('output')\n",
    "    open(output_file, \"w\").close()\n",
    "    \n",
    "    for line in file:\n",
    "        \n",
    "        query_pack = line.split('\\t', 1)\n",
    "        \n",
    "        # Get the ranks after full-ranker and re-ranker\n",
    "        doc_ranks = process_query(query_pack)\n",
    "        \n",
    "        # Append the results to tsv\n",
    "        with open(output_file, 'a', newline='') as f:\n",
    "            tsv_writer = csv.writer(f, delimiter='\\t')\n",
    "            [tsv_writer.writerow(row) for row in doc_ranks]\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        if counter % 1 == 0:\n",
    "            print(f'Ranked {counter} queries')\n",
    "            \n",
    "        # Removing below break will run for all 7k queries\n",
    "        if counter==10:\n",
    "            break\n",
    "        \n",
    "print(f'Completed ranking {counter} queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6902a49a",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "73fe1668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ms_marco_eval import compute_metrics_from_files\n",
    "metrics = compute_metrics_from_files(path_to_reference = ground_truth_file, path_to_candidate = output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "317470c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MRR @10': 0.0005027971073816346, 'QueriesRanked': 10}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "source": [
    "## Transformers QA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 555kB/s]\n",
      "Downloading: 100%|██████████| 496M/496M [00:59<00:00, 8.28MB/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 1.71MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 1.30MB/s]\n",
      "Downloading: 100%|██████████| 772/772 [00:00<00:00, 159kB/s]\n",
      "Downloading: 100%|██████████| 79.0/79.0 [00:00<00:00, 13.1kB/s]\n",
      "Downloading: 100%|██████████| 230/230 [00:00<00:00, 76.9kB/s]\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "}\n",
    "res = nlp(QA_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'score': 0.20016932487487793,\n",
       " 'start': 59,\n",
       " 'end': 84,\n",
       " 'answer': 'gives freedom to the user'}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = [{\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "},\n",
    "{\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'Bystander is the one who is present but not taking part in a situation or event'\n",
    "}]\n",
    "res = nlp(QA_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'score': 0.20016932487487793,\n",
       "  'start': 59,\n",
       "  'end': 84,\n",
       "  'answer': 'gives freedom to the user'},\n",
       " {'score': 1.335284451897678e-07,\n",
       "  'start': 28,\n",
       "  'end': 78,\n",
       "  'answer': 'present but not taking part in a situation or event'}]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [row['answer'] for row in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ranks_filtered = [[1, 'what', 123, 'next', 0.23, 1],\n",
    "                        [1, 'what', 345, 'after', 0.19, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[1, 'what', 'gives freedom to the user', 123, 'next'],\n",
       " [1,\n",
       "  'what',\n",
       "  'present but not taking part in a situation or event',\n",
       "  345,\n",
       "  'after']]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "qa_result = []\n",
    "for (qd_set, ans) in zip(doc_ranks_filtered, answers):\n",
    "    qa_result.append([qd_set[0], qd_set[1], ans, qd_set[2], qd_set[3]])\n",
    "qa_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3613jvsc74a57bd0747d4379572b41fb9f0c3c4e8732a88f178aecd427cc98be48ee7d3522439bd3",
   "display_name": "Python 3.6.13 64-bit ('casl': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}